{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the pdf file/files. \n",
    "#reader = PdfReader('WEF_The_Global_Cooperation_Barometer_2024.pdf')\n",
    "reader = PdfReader('2023_GPT4All_Technical_Report.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6021\n"
     ]
    }
   ],
   "source": [
    "#reader\n",
    "\n",
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nY'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits. \n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 500,\n",
    "    length_function = len,\n",
    ")\n",
    "documents = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = \"WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "# # Local PDF file uploads\n",
    "# if local_path:\n",
    "#   loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "#   data = loader.load()\n",
    "# else:\n",
    "#   print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --q chromadb\n",
    "# !pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "#EMBED_MODEL = \"multi-qa-MiniLM-L6-cos-v1\" \n",
    "\n",
    "COLLECTION_NAME = \"GPT4_tech_report-1\" #chunk-1000 & overlap - 500\n",
    "# \"GPT4_tech_report-1\"  #chunk-200 & overlap - 20 - gave poor results\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monda\\LeetCode\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "     model_name=EMBED_MODEL\n",
    " )\n",
    "\n",
    "collection = client.create_collection(\n",
    "     name=COLLECTION_NAME,\n",
    "     embedding_function=embedding_func,\n",
    "     metadata={\"hnsw:space\": \"cosine\"},\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "     ids=[f\"id{i}\" for i in range(len(documents))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "     query_texts=[\"How data was collected and curated for GPT4All training?\"],\n",
    "     n_results=1,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5216152021796041]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results2 = collection.query(\n",
    "     query_texts=[\"who are the authors of the article?\"],\n",
    "     n_results=1,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['license. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results2['documents'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.805583207935756]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query_results2.keys()\n",
    "query_results2['distances'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#query = \"the cost of training the GPT4all model?\"\n",
    "#query = \"How was the model trained?\"\n",
    "query = \"considerations used for GPT4all? \"\n",
    "\n",
    "query_results3 = collection.query(\n",
    "     query_texts=[query],\n",
    "     n_results=1,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6191096918007521]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results3['distances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['curation and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results3['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summerize the document\"\n",
    "#query = \"How was the model trained?\"\n",
    "\n",
    "query_results4 = collection.query(\n",
    "     query_texts=[query],\n",
    "     n_results=1,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results4['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8126886227707455]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results4['distances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # Download model\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# # The sentences we'd like to compute similarity about\n",
    "# sentences = ['Python is an interpreted high-level general-purpose programming language.',\n",
    "#     'Python is dynamically-typed and garbage-collected.',\n",
    "#     'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "# # Get embeddings of sentences\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# # Compute similarities\n",
    "# sim = util.cos_sim(embeddings[0], embeddings[1])\n",
    "# print(\"{0:.4f}\".format(sim.tolist()[0][0])) # 0.6445\n",
    "# sim = util.cos_sim(embeddings[0], embeddings[2])\n",
    "# print(\"{0:.4f}\".format(sim.tolist()[0][0])) # 0.0365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
